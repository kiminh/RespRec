\begin{algorithm}[tbp]
\small
\caption{\small Doubly-Robust Variance-Regularized Training}
\label{alg:drvrt}
\begin{algorithmic}[1]
\State {\bfseries Input:} $\trueRatings^\obsBiasedPairs,\trueRatings^\obsUnbiasedPairs,\nEpoch,\nStep,\regularization,\ratingParam_0,\propensityParam_0,\errorParam_{0,0}$
\FOR{$\epoch=0,...,\nEpoch-1$}
  \FOR{$\step=0,...,\nStep-1$}
    \State Sample a mini-batch $\obsBiasedPairs_{\epoch,\step}\subset\obsBiasedPairs$
    \State $\errorParam_{\epoch,\step+1}\leftarrow\errorParam_\step-\learningRate\nabla_{\errorParam_{\epoch,\step}}\loss{EI}(\errorParam)$
  \ENDFOR
  \State $\errorParam_{\epoch+1,0}\leftarrow\errorParam_{\epoch,\nStep}$
  \State $\ratingParam_{\epoch+1},\propensityParam_{\epoch+1}\leftarrow\text{\textalg{Vrt}}(\trueRatings^\obsBiasedPairs,\trueRatings^\obsUnbiasedPairs,\nStep,\regularization,\ratingParam_{\epoch},\propensityParam_{\epoch},\errorParam_{\epoch+1,0})$
\ENDFOR
\State {\bfseries Output:} $\ratingParam_\nEpoch,\propensityParam_\nEpoch,\errorParam_{\nEpoch,0}$
\end{algorithmic}
\end{algorithm}


\begin{algorithm}[tbp]
\small
\caption{\small \textalg{Vrt}: Variance-Regularized Training}
\label{alg:vrt}
\begin{algorithmic}[1]
\State {\bfseries Input:} $\trueRatings^\obsBiasedPairs,\trueRatings^\obsUnbiasedPairs,\nStep,\regularization,\ratingParam_0,\propensityParam_0,\errorParam_0$
\For{$\step=0,...,\nStep-1$}
  \State Sample mini-batches $\obsBiasedPairs_\step\subset\obsBiasedPairs$, $\obsUnbiasedPairs_\step\subset\obsUnbiasedPairs$, (and $\misBiasedPairs_\step\subset\misBiasedPairs$)
  \State Compute the inner loss $\loss{\innerMark}(\ratingParam_\step,\propensityParam_\step)$ on $\obsBiasedPairs_\step$ (and $\misBiasedPairs_\step$)
  \State Use AD to compute the gradients $\nabla_{\ratingParam_\step}\loss{\innerMark}(\ratingParam,\propensityParam_\step)$
  \State $\ratingParam_{\step+1}(\propensityParam_\step)\leftarrow\ratingParam_\step-\learningRate\nabla_{\ratingParam_\step}\loss{\innerMark}(\ratingParam,\propensityParam_\step)$
  \State Compute the outer loss $\loss{\rOuterMark}(\ratingParam_{\step+1}(\propensityParam_\step))$ on $\obsUnbiasedPairs$
  \State Use AD to compute the gradients $\nabla_{\propensityParam_\step}\loss{\rOuterMark}(\ratingParam_{\step+1}(\propensityParam))$
  \State $\propensityParam_{\step+1}\leftarrow\propensityParam_\step-\learningRate\nabla_{\propensityParam_\step}\loss{\rOuterMark}(\ratingParam_{\step+1}(\propensityParam))$
  \State Compute the inner loss $\loss{\innerMark}(\ratingParam_\step,\propensityParam_{\step+1})$ on $\obsBiasedPairs_\step$ (and $\misBiasedPairs_\step$)
  \State Use AD to compute the gradients $\nabla_{\ratingParam_\step}\loss{\innerMark}(\ratingParam,\propensityParam_{\step+1})$
  \State $\ratingParam_{\step+1}\leftarrow\ratingParam_\step-\learningRate\nabla_{\ratingParam_\step}\loss{\innerMark}(\ratingParam,\propensityParam_{\step+1})$
\EndFor
\State {\bfseries Output:} $\ratingParam_\nStep,\propensityParam_\nStep$
\end{algorithmic}
\end{algorithm}


Bi-level learning has been widely used in image segmentation~\cite{ochs2015bilevel}, multi-task learning~\cite{flamary2014learning}, hyper-parameter optimization~\cite{franceschi2017forward}, and learning to learn, etc.

\begin{figure}[!t]
\hspace*{-0.35cm}
\begin{minipage}[t]{1.05\linewidth}
\centering
\begin{figure}[H]
\centering
\begin{subfigure}{0.49\textwidth}
  \centering
  \includegraphics[height=3.84cm]{fig/mse_var_reg.eps}
  \caption{Vary regularization parameter $\regularization$.}
  \label{fig:mse var reg}
\end{subfigure}
\hspace{-0.45cm} % \hfill
\begin{subfigure}{0.49\textwidth}
  \centering
  \includegraphics[height=3.84cm]{fig/variance_epoch.eps}
  \caption{Propensity learned by NF-DR-LTD.}
  \label{fig:variance epoch}
\end{subfigure}
\caption{Effects of the variance regularizer on \textsc{Music}.}
\end{figure}%
\end{minipage}
\hspace*{-0.35cm}
\begin{minipage}[t]{1.05\linewidth}
\centering
\begin{figure}[H]
\centering
\begin{subfigure}{0.49\textwidth}
  \centering
  \includegraphics[height=3.54cm]{fig/propensity_rating.eps}
  \caption{Average propensity over ratings.}
  \label{fig:propensity rating}
\end{subfigure}
\hspace{-0.45cm} % \hfill
\begin{subfigure}{0.49\textwidth}
  \centering
  \includegraphics[height=3.54cm]{fig/propensity_epoch.eps}
  \caption{Set regularization parameter $\regularization=1$.}
  \label{fig:propensity epoch}
\end{subfigure}
\caption{Propensities estimated by NF-DR-LTD on \textsc{Music}.}
\end{figure}%
\end{minipage}
\hspace*{-0.35cm}
\begin{minipage}[t]{1.05\linewidth}
\centering
\begin{figure}[H]
\centering
\begin{subfigure}{0.49\textwidth}
  \centering
  \includegraphics[height=3.94cm]{fig/mse_unbiased_size.eps}
  \caption{MSE of the proposed approaches.}
  \label{fig:validation set accuracy}
\end{subfigure}
\hspace{-0.45cm} % \hfill
\begin{subfigure}{0.49\textwidth}
  \centering
  \includegraphics[height=3.92cm]{fig/drop_unbiased_size.eps}
  \caption{Comparison of two approaches.}
  \label{fig:validation set comparison}
\end{subfigure}
\caption{Effects of varying the validation set size on \textsc{Music}.}
\end{figure}%
\end{minipage}
\end{figure}



\begin{table*} [tbp]
% \small
% \footnotesize
\scriptsize
\centering
% \setlength\tabcolsep{3.0pt}
\caption{Average accuracy (MAE and MSE) with standard deviation (STD) over 10 different runs on unbiased and biased test sets.}
\label{tab:overall results}
\begin{threeparttable}
\begin{tabular}{l|cc|cc||cc|cc}
\toprule
& \multicolumn{4}{c||}{Evaluation results on unbiased test sets} & \multicolumn{4}{c}{Evaluation results on biased test sets} \\
\cmidrule(rl){2-9}
& \multicolumn{2}{c|}{\textsc{Music}} & \multicolumn{2}{c||}{\textsc{Coat}} & \multicolumn{2}{c|}{\textsc{Book}} & \multicolumn{2}{c}{\textsc{Movie}} \\
\cmidrule(rl){1-9}
Approach & MAE $\pm$ STD & MSE $\pm$ STD & MAE $\pm$ STD & MSE $\pm$ STD & MAE $\pm$ STD & MSE $\pm$ STD & MAE $\pm$ STD & MSE $\pm$ STD \\
% & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE \\
\midrule
MF & 1.167 $\pm$ 0.002 & 1.951 $\pm$ 0.003 & 0.948 $\pm$ 0.005 & 1.349 $\pm$ 0.007 & 0.609 $\pm$ 0.004 & 0.719 $\pm$ 0.006 & 0.604 $\pm$ 0.003 & 0.695 $\pm$ 0.005\\
NF & 1.034 $\pm$ 0.005 & 1.586 $\pm$ 0.007 & 0.919 $\pm$ 0.009 & 1.299 $\pm$ 0.013 & 0.571 $\pm$ 0.007 & 0.656 $\pm$ 0.013 & 0.587 $\pm$ 0.008 & 0.657 $\pm$ 0.014\\
\midrule
CPT-V & 0.914 $\pm$ 0.003 & 1.181 $\pm$ 0.004 & 0.992 $\pm$ 0.012 & 1.512 $\pm$ 0.020 & 0.798 $\pm$ 0.010 & 0.968 $\pm$ 0.016 & 0.775 $\pm$ 0.011 & 1.069 $\pm$ 0.017\\
PMF-NMAR & 1.190 $\pm$ 0.006 & 2.243 $\pm$ 0.010 & 0.910 $\pm$ 0.005 & 1.279 $\pm$ 0.009 & 0.595 $\pm$ 0.005 & 0.714 $\pm$ 0.009 & 0.601 $\pm$ 0.005 & 0.691 $\pm$ 0.009\\
MF-IPS & 0.857 $\pm$ 0.003 & 1.069 $\pm$ 0.005 & 0.891 $\pm$ 0.008 & 1.179 $\pm$ 0.012 & 0.635 $\pm$ 0.006 & 0.730 $\pm$ 0.010 & 0.615 $\pm$ 0.005 & 0.708 $\pm$ 0.008\\
NF-IPS & 0.842 $\pm$ 0.005 & 1.047 $\pm$ 0.006 & 0.863 $\pm$ 0.009 & 1.137 $\pm$ 0.011 & 0.582 $\pm$ 0.007 & 0.672 $\pm$ 0.011 & 0.591 $\pm$ 0.008 & 0.662 $\pm$ 0.012\\
MF-DR & 0.793 $\pm$ 0.002 & 1.037 $\pm$ 0.003 & 0.807 $\pm$ 0.004 & 1.058 $\pm$ 0.006 & 0.601 $\pm$ 0.003 & 0.697 $\pm$ 0.006 & 0.601 $\pm$ 0.003 & 0.680 $\pm$ 0.006\\
NF-DR & 0.782 $\pm$ 0.004 & 1.024 $\pm$ 0.007 & 0.784 $\pm$ 0.007 & 1.033 $\pm$ 0.013 & 0.562 $\pm$ 0.006 & 0.637 $\pm$ 0.013 & 0.581 $\pm$ 0.006 & 0.636 $\pm$ 0.010\\
\midrule
MF-IPS-LTD & 0.836 $\pm$ 0.002 & 1.009 $\pm$ 0.003 & 0.827 $\pm$ 0.004 & 1.062 $\pm$ 0.006 & 0.600 $\pm$ 0.004 & 0.682 $\pm$ 0.006 & 0.591 $\pm$ 0.003 & 0.661 $\pm$ 0.005\\
NF-IPS-LTD & 0.827 $\pm$ 0.003 & 0.992 $\pm$ 0.006 & 0.812 $\pm$ 0.006 & 1.041 $\pm$ 0.009 & 0.558 $\pm$ 0.006 & 0.634 $\pm$ 0.011 & 0.575 $\pm$ 0.006 & 0.629 $\pm$ 0.012\\
MF-DR-LTD & 0.781 $\pm$ 0.001 & 0.982 $\pm$ 0.002 & 0.770 $\pm$ 0.002 & 0.982 $\pm$ 0.003 & 0.572 $\pm$ 0.001 & 0.654 $\pm$ 0.004 & 0.588 $\pm$ 0.002 & 0.651 $\pm$ 0.004\\
NF-DR-LTD & \textbf{0.776} $\pm$ 0.002 & \textbf{0.977} $\pm$ 0.003 & \textbf{0.759} $\pm$ 0.004 & \textbf{0.973} $\pm$ 0.006 & \textbf{0.539} $\pm$ 0.003 & \textbf{0.598} $\pm$ 0.006 & \textbf{0.574} $\pm$ 0.003 & \textbf{0.619} $\pm$ 0.006\\
\bottomrule
\end{tabular}
\begin{tablenotes}
% \centering
\footnotesize
\item[*] Note that the bottom four rows, (MF/NF)-(IPS/DR)-LTD, summarize the evaluation results of the proposed approaches.
\end{tablenotes}
\end{threeparttable}
\end{table*}


\begin{equation*}
\likelihood(\propensityParam)
=\nbObservation^{|\obsBiasedPairs|}
(1-\nbObservation)^{|\allPairs\setminus\obsBiasedPairs|}
+\prod_{u,i\in\obsBiasedPairs}\nbCondition_{\trueRating}
+\prod_{u,i\in\obsUnbiasedPairs}\nbRating_{\trueRating}
\end{equation*}%


Given the biased set, $\nbObservation$ and $\vectorize{\nbCondition}$ are estimated by maximizing the likelihoods
\begin{equation*}
\begin{aligned}
\likelihood(\nbObservation)
&=\prod_{u,i\in\biasedPairs}\nbObservation
\prod_{u,i\in\allPairs\setminus\biasedPairs}(1-\nbObservation)
\text{ and}\\
\likelihood(\vectorize{\nbCondition})
&=\prod_{u,i\in\biasedPairs}\prod_{\scale\in\scales}\nbCondition_\scale^{\llbracket\trueRating=\scale\rrbracket}\,,
\end{aligned}
\end{equation*}%
where $\llbracket\cdot\rrbracket$ is the Iverson bracket that evaluates to 1 when the statement enclosed is true and to 0 otherwise.


Given the unbiased set, $\vectorize{\nbRating}$ is estimated by maximizing the likelihood
\begin{equation*}
\likelihood(\vectorize{\nbRating})=\prod_{u,i\in\unbiasedPairs}\prod_{\scale\in\scales}\nbRating_\scale^{\llbracket\trueRating=\scale\rrbracket}\,.
\end{equation*}%


\begin{table} [tbp]
% \small
\scriptsize
\centering
\caption{Characteristics of the datasets used in experiments.}
\label{tab:dataset characteristics}
\begin{tabular}{l|l|r|r|r|r}
\toprule
Test Set & Dataset Name & \#Users & \#Items & \#Ratings & Sparsity \\
\midrule
\multirow{2}{*}{Unbiased} & \textsc{Music} & 15,400 & 1,000 & 365,704 & 2.37\% \\
& \textsc{Coat} & 290 & 300 & 11,600 & 13.33\% \\
\midrule
\multirow{2}{*}{Biased} & \textsc{Book} & 19,804 & 22,086 & 1,273,679 & 0.29\% \\
& \textsc{Movie} & 6,040 & 3,706 & 1,000,209 & 4.47\% \\
\bottomrule
\end{tabular}
\end{table}